import os
import time
import json
import regex
from collections import defaultdict
from typing import Dict, List, Tuple, Set
from pathlib import Path

def run_train_bpe(
    input_path: str | os.PathLike,
    vocab_size: int,
    special_tokens: list[str],
    **kwargs,
) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
    """è®­ç»ƒBPEåˆ†è¯å™¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    print(f"ğŸš€ å¼€å§‹BPEè®­ç»ƒ")
    print(f"ğŸ“‹ é…ç½®: vocab_size={vocab_size}, special_tokens={special_tokens}")
    print(f"ğŸ“ æ•°æ®è·¯å¾„: {input_path}")
    
    # 1. åˆå§‹åŒ–è¯æ±‡è¡¨
    vocab: Dict[int, bytes] = {i: bytes([i]) for i in range(256)}
    current_next_id: int = 256
    existing_byte_values: Set[bytes] = set(vocab.values())
    
    # 2. æ·»åŠ ç‰¹æ®Štoken
    for st_str in special_tokens:
        if len(vocab) >= vocab_size:
            break
        st_bytes = st_str.encode("utf-8")
        if st_bytes not in existing_byte_values:
            vocab[current_next_id] = st_bytes
            existing_byte_values.add(st_bytes)
            current_next_id += 1
    
    # 3. åŠ è½½æ•°æ® (é‡‡æ ·ä»¥åŠ å¿«è®­ç»ƒ)
    print("ğŸ“– åŠ è½½è®­ç»ƒæ•°æ®...")
    try:
        with open(input_path, "r", encoding="utf-8", errors="ignore") as f:
            # è¯»å–å‰50MBçš„æ•°æ®è¿›è¡Œè®­ç»ƒ
            text = f.read(50 * 1024 * 1024)
    except FileNotFoundError:
        text = ""
    
    # 4. é¢„åˆ†è¯
    print("ğŸ”§ è¿›è¡Œé¢„åˆ†è¯...")
    chunks = regex.split('|'.join(map(regex.escape, special_tokens)), text)
    PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
    
    token_frequency_table = defaultdict(int)
    for chunk in chunks:
        for word in regex.findall(PAT, chunk):
            word_bytes = word.encode("utf-8")
            bytes_list = [bytes([x]) for x in word_bytes]
            token_frequency_table[tuple(bytes_list)] += 1
    
    print(f"âœ… é¢„åˆ†è¯å®Œæˆï¼Œå¾—åˆ° {len(token_frequency_table):,} ä¸ªå”¯ä¸€token")
    
    # 5. ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘ç‡
    print("ğŸ“Š ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘ç‡...")
    pair_counts = defaultdict(int)
    for token, freq in token_frequency_table.items():
        for i in range(len(token) - 1):
            pair_counts[token[i], token[i+1]] += freq
    
    print(f"âœ… ç»Ÿè®¡å®Œæˆï¼Œå‘ç° {len(pair_counts):,} ä¸ªå­—ç¬¦å¯¹")
    
    merges: List[Tuple[bytes, bytes]] = []
    
    # 6. BPEè®­ç»ƒä¸»å¾ªç¯
    print(f"ğŸ”„ å¼€å§‹BPEè®­ç»ƒï¼Œç›®æ ‡è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    total_merges_needed = vocab_size - len(vocab)
    
    iteration = 0
    while len(vocab) < vocab_size and pair_counts:
        iteration += 1
        if iteration % 100 == 0:
            print(f"   è¿›åº¦: {len(vocab)}/{vocab_size} tokens, {len(merges)} merges")
        
        # æ‰¾åˆ°é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹
        if not pair_counts:
            break
            
        max_count = max(pair_counts.values())
        candidates = [k for k, v in pair_counts.items() if v == max_count]
        best_pair = max(candidates)  # é€‰æ‹©å­—èŠ‚åºæœ€å¤§çš„
        
        # è®°å½•åˆå¹¶
        merges.append(best_pair)
        new_token_bytes = best_pair[0] + best_pair[1]
        
        # æ·»åŠ åˆ°è¯æ±‡è¡¨
        vocab[current_next_id] = new_token_bytes
        current_next_id += 1
        
        # æ›´æ–°å—å½±å“çš„token
        affected_tokens = []
        for token, freq in list(token_frequency_table.items()):
            has_pair = any(token[i:i+2] == best_pair for i in range(len(token) - 1))
            if has_pair:
                affected_tokens.append((token, freq))
        
        # å¤„ç†å—å½±å“çš„token
        for token, freq in affected_tokens:
            # ç§»é™¤æ—§çš„å­—ç¬¦å¯¹è®¡æ•°
            for i in range(len(token) - 1):
                pair_counts[token[i], token[i+1]] -= freq
                if pair_counts[token[i], token[i+1]] <= 0:
                    del pair_counts[token[i], token[i+1]]
            
            # åˆå¹¶å­—ç¬¦å¯¹
            new_token_frequency_seq = merge_token_sequence(token, best_pair, new_token_bytes)
            
            # æ·»åŠ æ–°çš„å­—ç¬¦å¯¹è®¡æ•°
            for i in range(len(new_token_frequency_seq) - 1):
                pair = (new_token_frequency_seq[i], new_token_frequency_seq[i+1])
                pair_counts[pair] += freq
            
            # æ›´æ–°tokené¢‘ç‡è¡¨
            del token_frequency_table[token]
            token_frequency_table[new_token_frequency_seq] += freq
    
    print(f"âœ… BPEè®­ç»ƒå®Œæˆ!")
    return vocab, merges

def merge_token_sequence(token: Tuple[bytes, ...], pair: Tuple[bytes, bytes], new_token: bytes) -> Tuple[bytes, ...]:
    """åˆå¹¶tokenåºåˆ—ä¸­çš„å­—ç¬¦å¯¹"""
    new_seq = []
    i = 0
    while i < len(token):
        if i < len(token) - 1 and token[i:i+2] == pair:
            new_seq.append(new_token)
            i += 2
        else:
            new_seq.append(token[i])
            i += 1
    return tuple(new_seq)

def save_vocab_and_merges(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], vocab_path: str, merges_path: str):
    """ä¿å­˜è¯æ±‡è¡¨å’Œåˆå¹¶åˆ—è¡¨åˆ°æ–‡ä»¶"""
    # 1. ä¿å­˜è¯æ±‡è¡¨ (JSONæ ¼å¼)
    vocab_str = {str(idx): token.decode('utf-8', errors='replace') for idx, token in vocab.items()}
    with open(vocab_path, 'w', encoding='utf-8') as f:
        json.dump(vocab_str, f, ensure_ascii=False, indent=2)
    
    # 2. ä¿å­˜åˆå¹¶åˆ—è¡¨ (æ–‡æœ¬æ ¼å¼)
    with open(merges_path, 'w', encoding='utf-8') as f:
        for merge in merges:
            part1 = merge[0].decode('utf-8', errors='replace')
            part2 = merge[1].decode('utf-8', errors='replace')
            f.write(f"{part1} {part2}\n")

def analyze_tokenizer_results(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], training_time: float):
    """åˆ†æåˆ†è¯å™¨ç»“æœå¹¶æŠ¥å‘Šæ‰€éœ€æŒ‡æ ‡"""
    print("\n" + "="*60)
    print("ğŸ“Š BPEè®­ç»ƒç»“æœåˆ†æ")
    print("="*60)
    
    # 1. è®­ç»ƒæ—¶é—´
    print(f"â±ï¸  è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    
    # 2. å†…å­˜ä½¿ç”¨
    try:
        import psutil
        process = psutil.Process()
        mem_usage = process.memory_info().rss / (1024 ** 3)  # GB
        print(f"ğŸ’¾ å³°å€¼å†…å­˜ä½¿ç”¨: {mem_usage:.2f} GB")
    except ImportError:
        print("ğŸ’¾ å³°å€¼å†…å­˜ä½¿ç”¨: æ— æ³•è·å– (psutilæœªå®‰è£…)")
    
    # 3. è¯æ±‡è¡¨ç»Ÿè®¡
    print(f"ğŸ“š è¯æ±‡è¡¨å¤§å°: {len(vocab):,}")
    print(f"ğŸ”— åˆå¹¶æ“ä½œæ•°: {len(merges):,}")
    
    # 4. æœ€é•¿tokenåˆ†æ
    longest_token = max(vocab.values(), key=len)
    longest_token_str = longest_token.decode('utf-8', errors='replace')
    print(f"ğŸ“ æœ€é•¿token: '{longest_token_str}' (é•¿åº¦: {len(longest_token)} å­—èŠ‚)")
    
    # 5. åˆ†ææœ€é•¿tokenæ˜¯å¦åˆç†
    if len(longest_token) > 20:
        print("ğŸ¤” æœ€é•¿tokenåˆ†æ: é•¿åº¦è¾ƒé•¿ï¼Œå¯èƒ½åŒ…å«å®Œæ•´çš„å•è¯æˆ–çŸ­è¯­")
    elif len(longest_token) > 10:
        print("âœ… æœ€é•¿tokenåˆ†æ: é•¿åº¦é€‚ä¸­ï¼Œå¯èƒ½åŒ…å«å¸¸è§å­è¯")
    else:
        print("âœ… æœ€é•¿tokenåˆ†æ: é•¿åº¦è¾ƒçŸ­ï¼Œç¬¦åˆBPEé¢„æœŸ")
    
    # 6. ç‰¹æ®Štokenæ£€æŸ¥
    special_tokens = [token for token in vocab.values() if b'<|' in token]
    print(f"ğŸ¯ ç‰¹æ®Štokenæ•°é‡: {len(special_tokens)}")
    for token in special_tokens:
        print(f"   - '{token.decode('utf-8', errors='replace')}'")
    
    return {
        'training_time': training_time,
        'vocab_size': len(vocab),
        'merges_count': len(merges),
        'longest_token': longest_token_str,
        'longest_token_length': len(longest_token)
    }

if __name__ == "__main__":
    # é…ç½®å‚æ•°
    config = {
        "vocab_size": 10000,
        "special_tokens": ["<|endoftext|>"],
    }
    
    # æ•°æ®é›†è·¯å¾„
    train_path = "../../data/TinyStoriesV2-GPT4-train.txt"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(train_path).exists():
        raise FileNotFoundError(f"è®­ç»ƒé›†æ–‡ä»¶ {train_path} ä¸å­˜åœ¨")
    
    # è®­ç»ƒæ¨¡å‹
    start_time = time.time()
    train_vocab, train_merges = run_train_bpe(train_path, **config)
    training_time = time.time() - start_time
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: {training_time:.2f}ç§’")
    
    # åˆ†æç»“æœ
    results = analyze_tokenizer_results(train_vocab, train_merges, training_time)
    
    # ä¿å­˜ç»“æœåˆ°ç£ç›˜
    output_dir = "."
    os.makedirs(output_dir, exist_ok=True)
    
    vocab_path = os.path.join(output_dir, "vocab.json")
    merges_path = os.path.join(output_dir, "merges.txt")
    
    save_vocab_and_merges(train_vocab, train_merges, vocab_path, merges_path)
    print(f"\nğŸ’¾ æ–‡ä»¶ä¿å­˜:")
    print(f"   - è¯æ±‡è¡¨: {vocab_path}")
    print(f"   - åˆå¹¶åˆ—è¡¨: {merges_path}")
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "="*60)
    print("ğŸ¯ ä»»åŠ¡å®Œæˆæ€»ç»“")
    print("="*60)
    print(f"âœ… æˆåŠŸè®­ç»ƒäº†å­—èŠ‚çº§BPEåˆ†è¯å™¨")
    print(f"âœ… è¯æ±‡è¡¨å¤§å°: {results['vocab_size']:,}")
    print(f"âœ… åŒ…å«ç‰¹æ®Štoken: <|endoftext|>")
    print(f"âœ… è®­ç»ƒæ—¶é—´: {results['training_time']:.2f}ç§’")
    print(f"âœ… æœ€é•¿token: '{results['longest_token']}' (é•¿åº¦: {results['longest_token_length']})")
    print(f"âœ… ç»“æœå·²åºåˆ—åŒ–åˆ°ç£ç›˜")
    
    # åˆ¤æ–­æ˜¯å¦åˆç†
    if results['training_time'] < 120:  # 2åˆ†é’Ÿ
        print("âœ… è®­ç»ƒæ—¶é—´åˆç†: ç¬¦åˆ<2åˆ†é’Ÿè¦æ±‚")
    else:
        print("âš ï¸  è®­ç»ƒæ—¶é—´è¾ƒé•¿: è¶…è¿‡2åˆ†é’Ÿè¦æ±‚")
        
    if results['longest_token_length'] > 50:
        print("ğŸ¤” æœ€é•¿tokenè¾ƒé•¿: å¯èƒ½éœ€è¦æ£€æŸ¥æ˜¯å¦åˆç†")
    else:
        print("âœ… æœ€é•¿tokené•¿åº¦åˆç†: ç¬¦åˆBPEé¢„æœŸ")
