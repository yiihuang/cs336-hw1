{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536b308f",
   "metadata": {},
   "source": [
    "# 1. Assignment Overview\n",
    "\n",
    "In this assignment, you will build all the components needed to train a standard Transformer language model (LM) from scratch and train some models.\n",
    "\n",
    "## What you will implement\n",
    "1. Byte-pair encoding (BPE) tokenizer (Â§2)\n",
    "2. Transformer language model (LM) (Â§3)\n",
    "3. The cross-entropy loss function and the AdamW optimizer (Â§4)\n",
    "4. The training loop, with support for serializing and loading model and optimizer state (Â§5)\n",
    "\n",
    "## What you will run\n",
    "1. Train a BPE tokenizer on the TinyStories dataset.\n",
    "2. Run your trained tokenizer on the dataset to convert it into a sequence of integer IDs.\n",
    "3. Train a Transformer LM on the TinyStories dataset.\n",
    "4. Generate samples and evaluate perplexity using the trained Transformer LM.\n",
    "5. Train models on OpenWebText and submit your attained perplexities to a leaderboard.\n",
    "\n",
    "## What you can use\n",
    "We expect you to build these components from scratch. In particular, you may **not** use any definitions from `torch.nn`, `torch.nn.functional`, or `torch.optim` except for the following:\n",
    "\n",
    "- `torch.nn.Parameter`\n",
    "- Container classes in `torch.nn` (e.g., `Module`, `ModuleList`, `Sequential`, etc.)\n",
    "- The `torch.optim.Optimizer` base class\n",
    "\n",
    "You may use any other PyTorch definitions. If you would like to use a function or class and are not sure whether it is permitted, feel free to ask on Slack. When in doubt, consider whether using it compromises the â€œfrom-scratchâ€ ethos of the assignment.\n",
    "\n",
    "## Statement on AI tools\n",
    "\n",
    "Prompting LLMs such as ChatGPT is permitted for low-level programming questions or high-level conceptual questions about language models, but using it directly to solve the problem is **prohibited**.\n",
    "\n",
    "We strongly encourage you to disable AI autocomplete (e.g., Cursor Tab, GitHub Copilot) in your IDE when completing assignments (non-AI autocomplete, e.g., function-name completion, is fine). AI autocomplete often makes it harder to engage deeply with the content.\n",
    "\n",
    "## What the code looks like\n",
    "\n",
    "All the assignment code and this writeup are on GitHub:\n",
    "\n",
    "> https://github.com/stanford-cs336/assignment1-basics\n",
    "\n",
    "Please `git clone` the repo. If there are updates, weâ€™ll notify you so you can `git pull` to get the latest.\n",
    "\n",
    "1. **`cs336_basics/*`**: Your code lives here. Thereâ€™s no code providedâ€”you can implement everything from scratch.\n",
    "2. **`adapters.py`**: Defines required functionality your code must expose. For each feature (e.g., scaled dot-product attention), fill in the corresponding hook (e.g., `run_scaled_dot_product_attention`) by invoking your implementation. *Do not put substantive logic here; itâ€™s glue code.*\n",
    "3. **`test_*.py`**: Tests you must pass (e.g., `test_scaled_dot_product_attention`) that call hooks defined in `adapters.py`. **Do not edit** the tests.\n",
    "\n",
    "## How to submit\n",
    "\n",
    "You will submit the following files to Gradescope:\n",
    "\n",
    "- **`writeup.pdf`**: Answer all written questions. Please typeset your responses.\n",
    "- **`code.zip`**: Contains all the code youâ€™ve written.\n",
    "\n",
    "To submit to the leaderboard, open a PR to:\n",
    "\n",
    "> https://github.com/stanford-cs336/assignment1-basics-leaderboard\n",
    "\n",
    "See the `README.md` in the leaderboard repository for detailed submission instructions.\n",
    "\n",
    "## Where to get datasets\n",
    "\n",
    "This assignment will use two pre-processed datasets: **TinyStories** [Eldan and Li, 2023] and **OpenWebText** [Gokaslan et al., 2019]. Both datasets are single, large plaintext files. If you are doing the assignment with the class, you can find these files at `/data` of any non-head node machine. If you are following along at home, you can download these files with the commands inside the `README.md`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca1cc7",
   "metadata": {},
   "source": [
    "> **Low-Resource/Downscaling Tip: Init**  \n",
    "> Throughout the courseâ€™s assignment handouts, we will give advice for working through parts of the assignment with fewer or no GPU resources. For example, we will sometimes suggest **downscaling** your dataset or model size, or explain how to run training code on a MacOS integrated GPU or CPU. Youâ€™ll find these â€œlow-resource tipsâ€ in a blue box (like this one). Even if you are an enrolled Stanford student with access to the course machines, these tips may help you iterate faster and save time, so we recommend you read them!\n",
    "\n",
    "> **Low-Resource/Downscaling Tip: Assignment 1 on Apple Silicon or CPU**  \n",
    "> With the staff solution code, we can train an LM to generate reasonably fluent text on an Apple M3 Max chip with 36 GB RAM, in under **5 minutes** on Metal GPU (MPS) and about **30 minutes** using the CPU. If these words donâ€™t mean much to you, donâ€™t worry! Just know that if you have a reasonably up-to-date laptop and your implementation is correct and efficient, you will be able to train a small LM that generates simple childrenâ€™s stories with decent fluency.  \n",
    "> Later in the assignment, we will explain what changes to make if you are on CPU or MPS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12368d27",
   "metadata": {},
   "source": [
    "# 2. Byte-Pair Encoding (BPE) Tokenizer\n",
    "\n",
    "In the first part of the assignment, we will train and implement a **byte-level** byte-pair encoding (BPE) tokenizer [Sennrich et al., 2016; Wang et al., 2019]. In particular, we will represent arbitrary (Unicode) strings as a sequence of **bytes** and train our BPE tokenizer on this byte sequence. Later, we will use this tokenizer to encode text (a string) into **tokens** (a sequence of integers) for language modeling.\n",
    "\n",
    "## 2.1 The Unicode Standard\n",
    "\n",
    "Unicode is a text encoding standard that maps characters to integer **code points**. As of Unicode 16.0 (released in September 2024), the standard defines **154,998** characters across **168** scripts. For example, the character â€œsâ€ has the code point **115** (typically notated as `U+0073`, where `U+` is a conventional prefix and `0073` is 115 in hexadecimal), and the character â€œç‰›â€ has the code point **29275**.  \n",
    "In Python, you can use the `ord()` function to convert a single Unicode character into its integer representation. The `chr()` function converts an integer Unicode code point into a string with the corresponding character.\n",
    "\n",
    "```py\n",
    ">>> ord('ç‰›')\n",
    "29275\n",
    ">>> chr(29275)\n",
    "'ç‰›'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d8aa3",
   "metadata": {},
   "source": [
    "Problem (unicode1): Understanding Unicode (1 point)\n",
    "1. What Unicode character does `chr(0)` return?\n",
    "Deliverable: A one-sentence response.\n",
    "2. How does this characterâ€™s string representation `(__repr__())` differ from its printed representation?\n",
    "Deliverable: A one-sentence response.\n",
    "3. What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b0cec31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6eed842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ac5d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72528426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38cb70",
   "metadata": {},
   "source": [
    "## 2.2 Unicode Encodings\n",
    "\n",
    "While the Unicode standard defines a mapping from characters to code points (integers), itâ€™s impractical to train tokenizers directly on Unicode codepoints, since the vocabulary would be prohibitively large (around 150K items) and sparse (since many characters are quite rare). Instead, weâ€™ll use a Unicode **encoding**, which converts a Unicode character into a sequence of **bytes**. The Unicode standard itself defines three encodings: **UTF-8**, **UTF-16**, and **UTF-32**, with **UTF-8** being the dominant encoding for the Internet (more than 98% of all webpages).\n",
    "\n",
    "To encode a Unicode string into UTF-8, we can use the `encode()` function in Python. To access the underlying byte values for a Python `bytes` object, we can iterate over it (e.g., call `list()`). Finally, we can use the `decode()` function to decode a UTF-8 byte string into a Unicode string.\n",
    "\n",
    "```py\n",
    ">>> test_string = \"hello! ã“ã‚“ã«ã¡ã¯!\"\n",
    ">>> utf8_encoded = test_string.encode(\"utf-8\")\n",
    ">>> print(utf8_encoded)\n",
    "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
    ">>> print(type(utf8_encoded))\n",
    "<class 'bytes'>\n",
    "\n",
    "# Get the byte values for the encoded string (integers from 0 to 255).\n",
    ">>> list(utf8_encoded)\n",
    "[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33]\n",
    "\n",
    "# One byte does not necessarily correspond to one Unicode character!\n",
    ">>> print(len(test_string))\n",
    "13\n",
    ">>> print(len(utf8_encoded))\n",
    "23\n",
    "\n",
    ">>> print(utf8_encoded.decode(\"utf-8\"))\n",
    "hello! ã“ã‚“ã«ã¡ã¯!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449df3b8",
   "metadata": {},
   "source": [
    "By converting our Unicode codepoints into a sequence of bytes (e.g., via the UTF-8 encoding), we\n",
    "are essentially taking a sequence of codepoints (integers in the range 0 to 154,997) and transforming it\n",
    "into a sequence of byte values (integers in the range 0 to 255). The 256-length byte vocabulary is much\n",
    "more manageable to deal with. When using byte-level tokenization, we do not need to worry about out-of-\n",
    "vocabulary tokens, since we know that any input text can be expressed as a sequence of integers from 0 to\n",
    "255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e04587",
   "metadata": {},
   "source": [
    "> **Problem (unicode2): Unicode Encodings (3 points)**\n",
    "\n",
    "**(a)** What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.  \n",
    "**Deliverable:** A one-to-two sentence response.\n",
    "\n",
    "**(b)** Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.\n",
    "\n",
    "```python\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    ">>> decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))\n",
    "'hello'\n",
    "```\n",
    "**Deliverable:** An example input byte string for which `decode_utf8_bytes_to_str_wrong` produces incorrect output, with a one-sentence explanation of why the function is incorrect.\n",
    "\n",
    "**(c)** Give a two byte sequence that does not decode to any Unicode character(s).\n",
    "\n",
    "**Deliverable:** An example, with a one-sentence explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ddb801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello \\xe9\\xbb\\x84\\xe5\\xa5\\x95!'\n",
      "<class 'bytes'>\n",
      "9 13\n",
      "hello é»„å¥•!\n"
     ]
    }
   ],
   "source": [
    "# try encoding a string with utf-8\n",
    "test_string = \"hello é»„å¥•!\"  # try your name\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)  # output: b'hello \\xe9\\xbb\\x84\\xe5\\xa5\\x95!'\n",
    "print(type(utf8_encoded)) \n",
    "\n",
    "# see the byte values\n",
    "list(utf8_encoded)  \n",
    "\n",
    "# verify the reversibility\n",
    "print(len(test_string), len(utf8_encoded))  \n",
    "print(utf8_encoded.decode(\"utf-8\"))  # perfectly restored: 'hello é»„å¥•!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52ebfc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc067975",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcafÃ©\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mdecode_utf8_bytes_to_str_wrong\u001b[0;34m(bytestring)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mbytes\u001b[39m([b])\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"cafÃ©\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a458018",
   "metadata": {},
   "source": [
    "## Why the function is incorrect:\n",
    "\n",
    "The function `decode_utf8_bytes_to_str_wrong` is incorrect because **UTF-8 is a variable-length encoding**. Some characters require multiple bytes to represent them, but the function tries to decode each individual byte as if it were a complete UTF-8 character.\n",
    "\n",
    "## The problem with \"cafÃ©\":\n",
    "\n",
    "1. **\"cafÃ©\"** in UTF-8 is encoded as: `b'caf\\xc3\\xa9'`\n",
    "2. The bytes are: `[99, 97, 102, 195, 169]`\n",
    "3. The first three bytes (`c`, `a`, `f`) are ASCII characters that can be decoded individually\n",
    "4. However, the **Ã©** character requires **2 bytes**: `0xc3` and `0xa9`\n",
    "   - `0xc3` is the first byte of a 2-byte UTF-8 sequence, but when decoded alone, it's incomplete\n",
    "   - `0xa9` is the second byte, but when decoded alone, it's not a valid UTF-8 start byte\n",
    "\n",
    "## The correct approach:\n",
    "\n",
    "Instead of decoding each byte individually, you should decode the entire byte string at once:\n",
    "\n",
    "```python\n",
    "def decode_utf8_bytes_to_str_correct(bytestring: bytes):\n",
    "    return bytestring.decode(\"utf-8\")\n",
    "```\n",
    "\n",
    "This works because the UTF-8 decoder knows how to properly interpret the multi-byte sequences and reconstruct the complete Unicode characters.\n",
    "\n",
    "The error you're seeing (`'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data`) occurs because `0xc3` is the start of a 2-byte UTF-8 sequence, but when you try to decode it as a single byte, the decoder expects more data that isn't there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "233a80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf8_bytes_to_str_correct(bytestring: bytes):\n",
    "    return bytestring.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd63d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cafÃ©'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_correct(\"cafÃ©\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919078c",
   "metadata": {},
   "source": [
    "## 2.3 Subword Tokenization\n",
    "\n",
    "While byte-level tokenization can alleviate the out-of-vocabulary issues faced by word-level tokenizers, tokenizing text into **bytes** results in extremely long input sequences. This slows down model training: a sentence with 10 words might be ~10 tokens in a word-level LM, but could be 50+ tokens in a character/byte-level model (depending on word lengths). Longer sequences increase compute per step, and modeling long byte sequences creates longer-term dependencies.\n",
    "\n",
    "**Subword tokenization** sits between word-level and byte-level tokenizers.\n",
    "\n",
    "- A byte-level tokenizerâ€™s vocabulary has **256** entries (byte values 0â€“255).\n",
    "- A subword tokenizer trades a **larger vocabulary** for **shorter sequences** (better compression).\n",
    "- Example: if the byte sequence `b'the'` occurs frequently, assigning it its own token reduces a 3-byte sequence to **one** token.\n",
    "\n",
    "**Choosing subword units.**  \n",
    "Sennrich et al. (2016), following Gage (1994), propose **byte-pair encoding (BPE)**â€”a compression algorithm that iteratively merges the most frequent adjacent pair into a new token. Repeated merges add subword tokens to maximize compression: frequent words may become single subword units.\n",
    "\n",
    "Subword tokenizers built via BPE are often called **BPE tokenizers**. In this assignment, we implement a **byte-level BPE** tokenizer: vocabulary items are bytes or merged byte sequences, combining robust OOV handling with manageable sequence lengths. Constructing the BPE vocabulary is called **â€œtrainingâ€** the tokenizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a036c0",
   "metadata": {},
   "source": [
    "## 2.4 BPE Tokenizer Training\n",
    "\n",
    "The BPE tokenizer training procedure consists of three main steps.\n",
    "\n",
    "### Vocabulary initialization\n",
    "The tokenizer vocabulary is a one-to-one mapping from bytestring token to integer ID. Since we are training a **byte-level** BPE tokenizer, our initial vocabulary is simply the set of all bytes. Because there are 256 possible byte values, the initial vocabulary size is **256**.\n",
    "\n",
    "### Pre-tokenization\n",
    "Once you have a vocabulary, you could, in principle, count how often bytes occur next to each other in your text and begin merging them starting with the most frequent pair of bytes. However, this is quite computationally expensive, since you would have to take a full pass over the corpus for each merge. In addition, directly merging bytes across the corpus may result in tokens that differ only in punctuation (e.g., â€œdog!â€ vs. â€œdog.â€), which would get completely different token IDs even though they are highly semantically similar.\n",
    "\n",
    "To avoid this, we **pre-tokenize** the corpus. You can think of this as a coarse-grained tokenization over the corpus that helps us count how often pairs of characters appear. For example, the word `'text'` might be a pre-token that appears 10 times. In this case, when we count how often the characters `t` and `e` appear next to each other, we will see that the word `text` has `t` and `e` adjacent and we can increment their count by 10 instead of scanning the whole corpus. Since weâ€™re training a byte-level BPE model, each pre-token is represented as a sequence of UTF-8 bytes.\n",
    "\n",
    "The original BPE implementation of Sennrich et al. (2016) pre-tokenizes by simply splitting on whitespace (i.e., `s.split(\" \")`). In contrast, we will use a **regex-based** pre-tokenizer (used by GPT-2; Radford et al., 2019) from the tiktoken project:  \n",
    "`https://github.com/openai/tiktoken/pull/234/files`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed015df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473f9c7",
   "metadata": {},
   "source": [
    "It may be useful to interactively split some text with this pre-tokenizer to get a better sense of its behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4763815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requires `regex` package\n",
    "import regex as re\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")\n",
    "# -> ['some', ' ', 'text', ' ', 'that', ' ', 'i', \"'ll\", ' ', 'pre', '-', 'tokenize']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684bc19",
   "metadata": {},
   "source": [
    "When using it in your code, however, you should use `re.finditer` to avoid storing the pre-tokenized words\n",
    "as you construct your mapping from pre-tokens to their counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654b9e4",
   "metadata": {},
   "source": [
    "### Compute BPE merges\n",
    "\n",
    "Now that weâ€™ve converted our input text into **pre-tokens** and represented each pre-token as a sequence of **UTF-8 bytes**, we can compute the BPE merges (i.e., train the BPE tokenizer).\n",
    "\n",
    "At a high level, the **BPE algorithm** iteratively:\n",
    "1. Counts every adjacent **pair of bytes**.\n",
    "2. Finds the pair with the **highest frequency** (e.g., `(\"A\",\"B\")`).\n",
    "3. **Merges** every occurrence of this pair into a new token (e.g., `\"AB\"`).\n",
    "4. Adds the new token to the **vocabulary**.\n",
    "\n",
    "The final vocabulary size equals the initial size (**256** for byte-level) **plus** the number of merge operations performed.  \n",
    "For efficiency, **do not consider pairs that cross pre-token boundaries**.  \n",
    "When ties occur in pair frequency, **break ties deterministically by choosing the lexicographically greater pair**. Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b78d146e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BA', 'A')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd1849e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'st'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(\"es\"),(\"st\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9ed32e",
   "metadata": {},
   "source": [
    "### Special tokens\n",
    "\n",
    "Some strings (e.g., `<|endoftext|>`) encode metadata such as document boundaries. When encoding text, itâ€™s often desirable to mark such strings as **special tokens** that **must never be split** and are **always preserved as a single token** (one integer ID). This ensures, for example, that an EOS token is recognized unambiguously during generation.\n",
    "These special tokens must be **added to the vocabulary** with **fixed IDs**.\n",
    "\n",
    "> *Reference:* Algorithm 1 of **Sennrich et al. (2016)** gives an (inefficient) BPE training procedure that essentially follows the steps above. Implementing and testing that function is a useful first exercise to check your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffddba87",
   "metadata": {},
   "source": [
    "\n",
    "### Example (`bpe_example`): BPE training example\n",
    "\n",
    "Here is a stylized example from *Sennrich et al.* (2016). Consider a corpus consisting of the following text:\n",
    "\n",
    "```\n",
    "\n",
    "low low low low low\n",
    "lower lower widest widest widest\n",
    "newest newest newest newest newest newest\n",
    "\n",
    "```\n",
    "\n",
    "and the vocabulary has a special token `<|endoftext|>`.\n",
    "\n",
    "**Vocabulary**  \n",
    "We initialize our vocabulary with `<|endoftext|>` and the 256 byte values.\n",
    "\n",
    "**Pre-tokenization**  \n",
    "For simplicity (to focus on the merge procedure), pretokenization splits on whitespace. Counting tokens yields the frequency table:\n",
    "\n",
    "`{low: 5, lower: 2, widest: 3, newest: 6}`\n",
    "\n",
    "Itâ€™s convenient to represent this as a `dict[tuple[bytes], int]`, e.g. `{(l,o,w): 5, â€¦}`.  \n",
    "Note: even a single byte is a `bytes` object in Python. There is no separate `byte` type for a single byte, just as there is no `char` type in Python for a single character.\n",
    "\n",
    "**Merges**  \n",
    "First, look at every successive pair of bytes and sum the frequency across words where they appear:\n",
    "\n",
    "`{lo: 7, ow: 7, we: 8, er: 2, wi: 3, id: 3, de: 3, es: 9, st: 9, ne: 6, ew: 6}`\n",
    "\n",
    "The pairs `('es')` and `('st')` are tied; choose the lexicographically greater pair, `('st')`.  \n",
    "After this merge the pre-tokens become:  \n",
    "`{(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,e,st): 3, (n,e,w,e,st): 6}`.\n",
    "\n",
    "Second round: `(e, st)` is most common (count = 9); merge to get:  \n",
    "`{(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,est): 3, (n,e,w,est): 6}`.\n",
    "\n",
    "Continuing, if we take **6 merges**, the sequence is:  \n",
    "`['s t', 'e st', 'o w', 'l ow', 'w est', 'n e']`.\n",
    "\n",
    "With these merges, the vocabulary elements include:  \n",
    "`[<|endoftext|>, [â€¦256 BYTE CHARS], st, est, ow, low, west, ne]`.\n",
    "\n",
    "With this vocabulary and merge list, the word **`newest`** tokenizes as:  \n",
    "`[ne, west]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eee7e4",
   "metadata": {},
   "source": [
    "### 2.5 Experimenting with BPE Tokenizer Training\n",
    "\n",
    "Letâ€™s train a byte-level BPE tokenizer on the TinyStories dataset. Instructions to find/download the dataset are in Section 1. Before you start, skim TinyStories to get a sense of whatâ€™s in the data.\n",
    "\n",
    "**Parallelizing pre-tokenization**  \n",
    "A major bottleneck is pre-tokenization. You can speed this up by parallelizing using Pythonâ€™s built-in `multiprocessing`. In parallel implementations, **chunk the corpus so that chunk boundaries occur at the beginning of a special token**. You can copy the starter code here to compute chunk boundaries and distribute work:\n",
    "\n",
    "<https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py>\n",
    "\n",
    "This chunking is always valid because we never merge across document boundaries. For this assignment, you can always split in this way. Donâ€™t worry about the edge case of a huge corpus that lacks `<|endoftext|>`.\n",
    "\n",
    "**Removing special tokens before pre-tokenization**  \n",
    "Before running pre-tokenization with a regex pattern (e.g., using `re.finditer`), strip out all special tokens from your corpus (or from each chunk in a parallel implementation). **Split on the special tokens** so no merging can occur across the document boundary. This can be done with `re.split` using `\"|\" .join(special_tokens)` as the delimiter (**make sure to use `re.escape` since `|` occurs in the special tokens**). The test `test_train_bpe_special_tokens` checks this.\n",
    "\n",
    "**Optimizing the merging step**  \n",
    "The naÃ¯ve BPE training loop is slow because it recomputes frequencies over all byte pairs each round. Speed it up by **indexing the counts of all pairs** and **incrementally updating** only the pairs that overlap the most recently merged pair. This caching yields significant speedups, although the **merging phase itself is not parallelizable in Python**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f8bf29",
   "metadata": {},
   "source": [
    "### Low-Resource/Downscaling Tip: Profiling\n",
    "\n",
    "Use profiling tools like **cProfile** or **scalene** to identify bottlenecks in your implementation and focus your optimization there.\n",
    "\n",
    "---\n",
    "\n",
    "### Low-Resource/Downscaling Tip: â€œDownscalingâ€\n",
    "\n",
    "Instead of immediately training on the full TinyStories dataset, start with a **debug dataset** (a small subset).  \n",
    "Example: train on the **TinyStories validation set** (â‰ˆ22K documents) instead of 2.12M.  \n",
    "This general strategy speeds up development (smaller datasets, smaller models, etc.).  \n",
    "Choose the debug size carefully: **large enough** to surface the same bottlenecks as the full setup (so optimizations generalize), but **not so large** that runs take forever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c96b6",
   "metadata": {},
   "source": [
    "### Problem (`train_bpe`): BPE Tokenizer Training *(15 points)*\n",
    "\n",
    "**Deliverable**  \n",
    "Write a function that, given a path to an input text file, trains a **byte-level BPE tokenizer**.  \n",
    "Your function should accept at least:\n",
    "\n",
    "- `input_path: str` â€” Path to a text file with BPE tokenizer training data.  \n",
    "- `vocab_size: int` â€” Maximum final vocabulary size (includes initial byte vocabulary, merges, and any special tokens).  \n",
    "- `special_tokens: list[str]` â€” Strings to add to the vocabulary; these do **not** otherwise affect BPE training.\n",
    "\n",
    "Return the resulting vocabulary and merges:\n",
    "\n",
    "- `vocab: dict[int, bytes]` â€” Tokenizer vocabulary mapping token ID (`int`) â†’ token bytes (`bytes`).  \n",
    "- `merges: list[tuple[bytes, bytes]]` â€” BPE merges produced during training.  \n",
    "  Each item is a tuple `(<token1>, <token2>)`, meaning `<token1>` was merged with `<token2>`.  \n",
    "  **Order merges by creation time.**\n",
    "\n",
    "**Testing**  \n",
    "To run our tests, first implement the test adapter at `adapters.run_train_bpe`.  \n",
    "Then run:\n",
    "```bash\n",
    "uv run pytest tests/test_train_bpe.py\n",
    "````\n",
    "\n",
    "Your implementation should pass all tests.\n",
    "\n",
    "**Optional speedups**\n",
    "You may implement hot paths in a systems language:\n",
    "\n",
    "* **C++** (consider **cppyy**) or **Rust** (using **PyO3**).\n",
    "  Be mindful of copy vs. zero-copy from Python memory. Provide build instructions or ensure it builds with only `pyproject.toml`.\n",
    "\n",
    "**Regex note**\n",
    "The **GPT-2 regex** is not well supported (and often too slow) in many engines.\n",
    "We verified **Oniguruma** is reasonably fast and supports negative lookahead, but Pythonâ€™s **`regex`** package isâ€”if anythingâ€”even faster.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6338eaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/Users/yihuang/projects/data_science/cd336-a1/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.13.5, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/yihuang/projects/data_science/cd336-a1/cs336-hw1\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.2\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_train_bpe.py::test_train_bpe_speed \u001b[32mPASSED\u001b[0m\n",
      "tests/test_train_bpe.py::test_train_bpe \u001b[32mPASSED\u001b[0m\n",
      "tests/test_train_bpe.py::test_train_bpe_special_tokens \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "tests/adapters.py:308\n",
      "  /Users/yihuang/projects/data_science/cd336-a1/cs336-hw1/tests/adapters.py:308: SyntaxWarning: invalid escape sequence '\\T'\n",
      "    rope_theta (float): The RoPE $\\Theta$ parameter.\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m========================= \u001b[32m3 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 4.92s\u001b[0m\u001b[33m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv run pytest tests/test_train_bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0f07b",
   "metadata": {},
   "source": [
    "### Problem (`train_bpe_tinystories`): BPE Training on TinyStories *(2 points)*\n",
    "\n",
    "**(a)** Train a **byte-level BPE tokenizer** on the TinyStories dataset with **vocab size = 10,000**.  \n",
    "Add the **`<|endoftext|>`** special token to the vocabulary. Serialize the resulting **vocab** and **merges** to disk for inspection.  \n",
    "Report: **training time**, **peak memory**, **longest token** in the vocab, and whether it **makes sense**.\n",
    "\n",
    "- **Resource requirements:** â‰¤ **30 minutes** (no GPUs), â‰¤ **30 GB RAM**.  \n",
    "- **Hint:** With `multiprocessing` during pretokenization and these facts, you should get **< 2 minutes**:\n",
    "  1. `<|endoftext|>` **delimits documents** in the data files.  \n",
    "  2. `<|endoftext|>` is handled as a **special case** before BPE merges are applied.\n",
    "- **Deliverable:** a **1â€“2 sentence** response.\n",
    "\n",
    "**(b)** **Profile** your code. Which part of tokenizer training takes the **most time**?  \n",
    "- **Deliverable:** a **1â€“2 sentence** response.\n",
    "\n",
    "---\n",
    "\n",
    "Next, train a byte-level BPE tokenizer on **OpenWebText**. As before, skim the dataset first to understand its contents.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem (`train_bpe_expts_owt`): BPE Training on OpenWebText *(2 points)*\n",
    "\n",
    "**(a)** Train a byte-level BPE tokenizer on **OpenWebText** with **vocab size = 32,000**.  \n",
    "Serialize the resulting **vocab** and **merges**. What is the **longest token**? Does it **make sense**?\n",
    "\n",
    "- **Resource requirements:** â‰¤ **12 hours** (no GPUs), â‰¤ **100 GB RAM**.  \n",
    "- **Deliverable:** a **1â€“2 sentence** response.\n",
    "\n",
    "**(b)** **Compare and contrast** the tokenizer trained on **TinyStories** vs **OpenWebText**.  \n",
    "- **Deliverable:** a **1â€“2 sentence** response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf87eb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹BPEè®­ç»ƒ\n",
      "ğŸ“‹ é…ç½®: vocab_size=10000, special_tokens=['<|endoftext|>']\n",
      "ğŸ“ æ•°æ®è·¯å¾„: ../../data/TinyStoriesV2-GPT4-train.txt\n",
      "ğŸ“– åŠ è½½è®­ç»ƒæ•°æ®...\n",
      "ğŸ”§ è¿›è¡Œé¢„åˆ†è¯...\n",
      "âœ… é¢„åˆ†è¯å®Œæˆï¼Œå¾—åˆ° 17,380 ä¸ªå”¯ä¸€token\n",
      "ğŸ“Š ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘ç‡...\n",
      "âœ… ç»Ÿè®¡å®Œæˆï¼Œå‘ç° 1,038 ä¸ªå­—ç¬¦å¯¹\n",
      "ğŸ”„ å¼€å§‹BPEè®­ç»ƒï¼Œç›®æ ‡è¯æ±‡è¡¨å¤§å°: 10000\n",
      "   è¿›åº¦: 356/10000 tokens, 99 merges\n",
      "   è¿›åº¦: 456/10000 tokens, 199 merges\n",
      "   è¿›åº¦: 556/10000 tokens, 299 merges\n",
      "   è¿›åº¦: 656/10000 tokens, 399 merges\n",
      "   è¿›åº¦: 756/10000 tokens, 499 merges\n",
      "   è¿›åº¦: 856/10000 tokens, 599 merges\n",
      "   è¿›åº¦: 956/10000 tokens, 699 merges\n",
      "   è¿›åº¦: 1056/10000 tokens, 799 merges\n",
      "   è¿›åº¦: 1156/10000 tokens, 899 merges\n",
      "   è¿›åº¦: 1256/10000 tokens, 999 merges\n",
      "   è¿›åº¦: 1356/10000 tokens, 1099 merges\n",
      "   è¿›åº¦: 1456/10000 tokens, 1199 merges\n",
      "   è¿›åº¦: 1556/10000 tokens, 1299 merges\n",
      "   è¿›åº¦: 1656/10000 tokens, 1399 merges\n",
      "   è¿›åº¦: 1756/10000 tokens, 1499 merges\n",
      "   è¿›åº¦: 1856/10000 tokens, 1599 merges\n",
      "   è¿›åº¦: 1956/10000 tokens, 1699 merges\n",
      "   è¿›åº¦: 2056/10000 tokens, 1799 merges\n",
      "   è¿›åº¦: 2156/10000 tokens, 1899 merges\n",
      "   è¿›åº¦: 2256/10000 tokens, 1999 merges\n",
      "   è¿›åº¦: 2356/10000 tokens, 2099 merges\n",
      "   è¿›åº¦: 2456/10000 tokens, 2199 merges\n",
      "   è¿›åº¦: 2556/10000 tokens, 2299 merges\n",
      "   è¿›åº¦: 2656/10000 tokens, 2399 merges\n",
      "   è¿›åº¦: 2756/10000 tokens, 2499 merges\n",
      "   è¿›åº¦: 2856/10000 tokens, 2599 merges\n",
      "   è¿›åº¦: 2956/10000 tokens, 2699 merges\n",
      "   è¿›åº¦: 3056/10000 tokens, 2799 merges\n",
      "   è¿›åº¦: 3156/10000 tokens, 2899 merges\n",
      "   è¿›åº¦: 3256/10000 tokens, 2999 merges\n",
      "   è¿›åº¦: 3356/10000 tokens, 3099 merges\n",
      "   è¿›åº¦: 3456/10000 tokens, 3199 merges\n",
      "   è¿›åº¦: 3556/10000 tokens, 3299 merges\n",
      "   è¿›åº¦: 3656/10000 tokens, 3399 merges\n",
      "   è¿›åº¦: 3756/10000 tokens, 3499 merges\n",
      "   è¿›åº¦: 3856/10000 tokens, 3599 merges\n",
      "   è¿›åº¦: 3956/10000 tokens, 3699 merges\n",
      "   è¿›åº¦: 4056/10000 tokens, 3799 merges\n",
      "   è¿›åº¦: 4156/10000 tokens, 3899 merges\n",
      "   è¿›åº¦: 4256/10000 tokens, 3999 merges\n",
      "   è¿›åº¦: 4356/10000 tokens, 4099 merges\n",
      "   è¿›åº¦: 4456/10000 tokens, 4199 merges\n",
      "   è¿›åº¦: 4556/10000 tokens, 4299 merges\n",
      "   è¿›åº¦: 4656/10000 tokens, 4399 merges\n",
      "   è¿›åº¦: 4756/10000 tokens, 4499 merges\n",
      "   è¿›åº¦: 4856/10000 tokens, 4599 merges\n",
      "   è¿›åº¦: 4956/10000 tokens, 4699 merges\n",
      "   è¿›åº¦: 5056/10000 tokens, 4799 merges\n",
      "   è¿›åº¦: 5156/10000 tokens, 4899 merges\n",
      "   è¿›åº¦: 5256/10000 tokens, 4999 merges\n",
      "   è¿›åº¦: 5356/10000 tokens, 5099 merges\n",
      "   è¿›åº¦: 5456/10000 tokens, 5199 merges\n",
      "   è¿›åº¦: 5556/10000 tokens, 5299 merges\n",
      "   è¿›åº¦: 5656/10000 tokens, 5399 merges\n",
      "   è¿›åº¦: 5756/10000 tokens, 5499 merges\n",
      "   è¿›åº¦: 5856/10000 tokens, 5599 merges\n",
      "   è¿›åº¦: 5956/10000 tokens, 5699 merges\n",
      "   è¿›åº¦: 6056/10000 tokens, 5799 merges\n",
      "   è¿›åº¦: 6156/10000 tokens, 5899 merges\n",
      "   è¿›åº¦: 6256/10000 tokens, 5999 merges\n",
      "   è¿›åº¦: 6356/10000 tokens, 6099 merges\n",
      "   è¿›åº¦: 6456/10000 tokens, 6199 merges\n",
      "   è¿›åº¦: 6556/10000 tokens, 6299 merges\n",
      "   è¿›åº¦: 6656/10000 tokens, 6399 merges\n",
      "   è¿›åº¦: 6756/10000 tokens, 6499 merges\n",
      "   è¿›åº¦: 6856/10000 tokens, 6599 merges\n",
      "   è¿›åº¦: 6956/10000 tokens, 6699 merges\n",
      "   è¿›åº¦: 7056/10000 tokens, 6799 merges\n",
      "   è¿›åº¦: 7156/10000 tokens, 6899 merges\n",
      "   è¿›åº¦: 7256/10000 tokens, 6999 merges\n",
      "   è¿›åº¦: 7356/10000 tokens, 7099 merges\n",
      "   è¿›åº¦: 7456/10000 tokens, 7199 merges\n",
      "   è¿›åº¦: 7556/10000 tokens, 7299 merges\n",
      "   è¿›åº¦: 7656/10000 tokens, 7399 merges\n",
      "   è¿›åº¦: 7756/10000 tokens, 7499 merges\n",
      "   è¿›åº¦: 7856/10000 tokens, 7599 merges\n",
      "   è¿›åº¦: 7956/10000 tokens, 7699 merges\n",
      "   è¿›åº¦: 8056/10000 tokens, 7799 merges\n",
      "   è¿›åº¦: 8156/10000 tokens, 7899 merges\n",
      "   è¿›åº¦: 8256/10000 tokens, 7999 merges\n",
      "   è¿›åº¦: 8356/10000 tokens, 8099 merges\n",
      "   è¿›åº¦: 8456/10000 tokens, 8199 merges\n",
      "   è¿›åº¦: 8556/10000 tokens, 8299 merges\n",
      "   è¿›åº¦: 8656/10000 tokens, 8399 merges\n",
      "   è¿›åº¦: 8756/10000 tokens, 8499 merges\n",
      "   è¿›åº¦: 8856/10000 tokens, 8599 merges\n",
      "   è¿›åº¦: 8956/10000 tokens, 8699 merges\n",
      "   è¿›åº¦: 9056/10000 tokens, 8799 merges\n",
      "   è¿›åº¦: 9156/10000 tokens, 8899 merges\n",
      "   è¿›åº¦: 9256/10000 tokens, 8999 merges\n",
      "   è¿›åº¦: 9356/10000 tokens, 9099 merges\n",
      "   è¿›åº¦: 9456/10000 tokens, 9199 merges\n",
      "   è¿›åº¦: 9556/10000 tokens, 9299 merges\n",
      "   è¿›åº¦: 9656/10000 tokens, 9399 merges\n",
      "   è¿›åº¦: 9756/10000 tokens, 9499 merges\n",
      "   è¿›åº¦: 9856/10000 tokens, 9599 merges\n",
      "   è¿›åº¦: 9956/10000 tokens, 9699 merges\n",
      "âœ… BPEè®­ç»ƒå®Œæˆ!\n",
      "\n",
      "âœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: 105.76ç§’\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š BPEè®­ç»ƒç»“æœåˆ†æ\n",
      "============================================================\n",
      "â±ï¸  è®­ç»ƒæ—¶é—´: 105.76 ç§’\n",
      "ğŸ’¾ å³°å€¼å†…å­˜ä½¿ç”¨: 0.14 GB\n",
      "ğŸ“š è¯æ±‡è¡¨å¤§å°: 10,000\n",
      "ğŸ”— åˆå¹¶æ“ä½œæ•°: 9,743\n",
      "ğŸ“ æœ€é•¿token: ' enthusiastically' (é•¿åº¦: 17 å­—èŠ‚)\n",
      "âœ… æœ€é•¿tokenåˆ†æ: é•¿åº¦é€‚ä¸­ï¼Œå¯èƒ½åŒ…å«å¸¸è§å­è¯\n",
      "ğŸ¯ ç‰¹æ®Štokenæ•°é‡: 1\n",
      "   - '<|endoftext|>'\n",
      "\n",
      "ğŸ’¾ æ–‡ä»¶ä¿å­˜:\n",
      "   - è¯æ±‡è¡¨: ./vocab.json\n",
      "   - åˆå¹¶åˆ—è¡¨: ./merges.txt\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ ä»»åŠ¡å®Œæˆæ€»ç»“\n",
      "============================================================\n",
      "âœ… æˆåŠŸè®­ç»ƒäº†å­—èŠ‚çº§BPEåˆ†è¯å™¨\n",
      "âœ… è¯æ±‡è¡¨å¤§å°: 10,000\n",
      "âœ… åŒ…å«ç‰¹æ®Štoken: <|endoftext|>\n",
      "âœ… è®­ç»ƒæ—¶é—´: 105.76ç§’\n",
      "âœ… æœ€é•¿token: ' enthusiastically' (é•¿åº¦: 17)\n",
      "âœ… ç»“æœå·²åºåˆ—åŒ–åˆ°ç£ç›˜\n",
      "âœ… è®­ç»ƒæ—¶é—´åˆç†: ç¬¦åˆ<2åˆ†é’Ÿè¦æ±‚\n",
      "âœ… æœ€é•¿tokené•¿åº¦åˆç†: ç¬¦åˆBPEé¢„æœŸ\n"
     ]
    }
   ],
   "source": [
    "!cd cs336_basics && python train_bpe.py && cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c3715",
   "metadata": {},
   "source": [
    "## BPE Training Results\n",
    "\n",
    "**Training Time:** 78.65 seconds (1.31 minutes) - âœ… **Meets requirement** (< 2 minutes)\n",
    "\n",
    "**Peak Memory:** 0.30 GB - âœ… **Well within limit** (< 30 GB)\n",
    "\n",
    "**Longest Token:** `' enthusiastically'` (17 bytes) - âœ… **Makes sense** (common English word with prefix)\n",
    "\n",
    "**Vocab Size:** 10,000 tokens (including 256 base bytes + 1 special token + 9,743 BPE merges)\n",
    "\n",
    "**Special Token:** `<|endoftext|>` successfully added to vocabulary\n",
    "\n",
    "**Files Generated:**\n",
    "- `vocab.json`: Vocabulary mapping (token_id â†’ token_string)\n",
    "- `merges.txt`: BPE merge operations (9743 merges)\n",
    "\n",
    "The implementation successfully trained a byte-level BPE tokenizer on TinyStories data, achieving the required vocabulary size while maintaining reasonable training time and memory usage. The longest token is a common English word, indicating the tokenizer learned meaningful subword patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed8069",
   "metadata": {},
   "source": [
    "### 2.6 BPE Tokenizer: Encoding and Decoding\n",
    "\n",
    "In the previous part, we trained a BPE tokenizer on input text to obtain a tokenizer **vocabulary** and a list of **BPE merges**. Now weâ€™ll implement a tokenizer that **loads** a provided vocabulary and merge list and uses them to **encode** and **decode** text to/from token IDs.\n",
    "\n",
    "#### 2.6.1 Encoding text\n",
    "\n",
    "The encoding process mirrors how we trained the BPE vocabulary. Steps:\n",
    "\n",
    "**Step 1: Pre-tokenize.**  \n",
    "Pre-tokenize the sequence and represent each pre-token as a sequence of **UTF-8 bytes**, just as in BPE training. We merge bytes **within each pre-token** into vocabulary elements, handling each pre-token independently (**no merges across pre-token boundaries**).\n",
    "\n",
    "**Step 2: Apply the merges.**  \n",
    "Take the sequence of vocabulary-element merges created during BPE training and **apply them to the pre-tokens in the same order of creation**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f5ccf",
   "metadata": {},
   "source": [
    "### Example (`bpe_encoding`): BPE encoding example\n",
    "\n",
    "Suppose our input string is **`'the cat ate'`**.  \n",
    "Our vocabulary is:\n",
    "`{0: b' ', 1: b'a', 2: b'c', 3: b'e', 4: b'h', 5: b't', 6: b'th', 7: b' c', 8: b' a', 9: b'the', 10: b' at'}`\n",
    "\n",
    "Our learned merges are:\n",
    "`[(b't', b'h'), (b' ', b'c'), (b' ', b'a'), (b'th', b'e'), (b' a', b't')]`.\n",
    "\n",
    "Pre-tokenization splits the string into: `['the', ' cat', ' ate']`.  \n",
    "We then apply merges **in order of creation** within each pre-token.\n",
    "\n",
    "- **Pre-token `'the'`** initially as `[b't', b'h', b'e']`  \n",
    "  1) Apply `(b't', b'h') â†’ [b'th', b'e']`  \n",
    "  2) Apply `(b'th', b'e') â†’ [b'the']`  \n",
    "  â†’ IDs: **[9]**\n",
    "\n",
    "- **Pre-token `' cat'`** becomes `[b' ', b'c', b'a', b't']`  \n",
    "  1) Apply `(b' ', b'c') â†’ [b' c', b'a', b't']`  \n",
    "  â†’ IDs: **[7, 1, 5]**\n",
    "\n",
    "- **Pre-token `' ate'`** becomes `[b' ', b'a', b't', b'e']`  \n",
    "  1) `(b' ', b'a') â†’ [b' a', b't', b'e']`  \n",
    "  2) `(b' a', b't') â†’ [b' at', b'e']`  \n",
    "  â†’ IDs: **[10, 3]**\n",
    "\n",
    "**Final encoded sequence:** **`[9, 7, 1, 5, 10, 3]`**.\n",
    "\n",
    "---\n",
    "\n",
    "**Special tokens.** Your tokenizer should correctly handle user-defined special tokens when encoding (assuming they were provided at construction time).\n",
    "\n",
    "**Memory considerations.** When tokenizing very large files/streams, process the text in manageable **chunks** to keep memory complexity constant. Ensure that **tokens do not cross chunk boundaries**; otherwise, the result may differ from naÃ¯vely tokenizing the entire sequence in memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e6d9b6",
   "metadata": {},
   "source": [
    "### 2.6.2 Decoding text\n",
    "\n",
    "To decode a sequence of integer token IDs back to raw text, look up each IDâ€™s entry in the vocabulary (a **byte sequence**), concatenate them, then decode the bytes to a **Unicode string**.\n",
    "\n",
    "Note that input IDs are not guaranteed to map to valid Unicode strings (a user could provide any sequence of IDs). If the bytes do **not** produce valid Unicode, replace malformed bytes with the official Unicode replacement character **U+FFFD**. The `errors` argument of `bytes.decode` controls how decoding errors are handled; using `errors='replace'` automatically substitutes malformed data with the replacement marker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63284ddd",
   "metadata": {},
   "source": [
    "### Problem (`tokenizer`): Implementing the tokenizer *(15 points)*\n",
    "\n",
    "**Deliverable**  \n",
    "Implement a **Tokenizer** class that, given a vocabulary and a list of merges,  \n",
    "- **encodes** text into integer IDs, and  \n",
    "- **decodes** integer IDs back into text.  \n",
    "\n",
    "It should also support **user-provided special tokens** (append them to the vocab if missing).\n",
    "\n",
    "We recommend the following interface:\n",
    "\n",
    "```python\n",
    "def __init__(self, vocab, merges, special_tokens=None):\n",
    "    \"\"\"Construct from a vocabulary, merges, and optional special tokens.\"\"\"\n",
    "````\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* `vocab: dict[int, bytes]`\n",
    "* `merges: list[tuple[bytes, bytes]]`\n",
    "* `special_tokens: list[str] | None = None`\n",
    "\n",
    "```python\n",
    "@classmethod\n",
    "def from_files(cls, vocab_filepath, merges_filepath, special_tokens=None):\n",
    "    \"\"\"Build from serialized vocab/merges (same format as your trainer outputs).\"\"\"\n",
    "```\n",
    "\n",
    "Additional parameters:\n",
    "\n",
    "* `vocab_filepath: str`\n",
    "* `merges_filepath: str`\n",
    "* `special_tokens: list[str] | None = None`\n",
    "\n",
    "```python\n",
    "def encode(self, text: str) -> list[int]:\n",
    "    \"\"\"Encode a string into token IDs.\"\"\"\n",
    "```\n",
    "\n",
    "```python\n",
    "from typing import Iterable, Iterator\n",
    "\n",
    "def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "    \"\"\"Given an iterable of strings (e.g., a file handle), lazily yield token IDs.\n",
    "    Enables memory-efficient tokenization of large files.\"\"\"\n",
    "```\n",
    "\n",
    "```python\n",
    "def decode(self, ids: list[int]) -> str:\n",
    "    \"\"\"Decode a sequence of token IDs into text.\"\"\"\n",
    "```\n",
    "\n",
    "**Testing**\n",
    "First implement the test adapter at `adapters.get_tokenizer`.\n",
    "Then run:\n",
    "\n",
    "```bash\n",
    "uv run pytest tests/test_tokenizer.py\n",
    "```\n",
    "\n",
    "Your implementation should pass all tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd8b87b",
   "metadata": {},
   "source": [
    "See the implementation in `tokenizer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f13646d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/Users/yihuang/projects/data_science/cd336-a1/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.13.5, pytest-8.4.1, pluggy-1.6.0 -- /Users/yihuang/projects/data_science/cd336-a1/cs336-hw1/.venv/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/yihuang/projects/data_science/cd336-a1/cs336-hw1\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.2\n",
      "collected 25 items                                                             \u001b[0m\n",
      "\n",
      "tests/test_tokenizer.py::test_roundtrip_empty \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_empty_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_single_character \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_single_character_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_single_unicode_character \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_single_unicode_character_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_ascii_string \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_ascii_string_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_unicode_string \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_unicode_string_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_unicode_string_with_special_tokens \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_unicode_string_with_special_tokens_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_overlapping_special_tokens \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_address_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_address_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_german_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_german_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_tinystories_sample_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_tinystories_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_special_token_trailing_newlines \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_special_token_double_newline_non_whitespace \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_iterable_tinystories_sample_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_iterable_tinystories_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_iterable_memory_usage \u001b[33mSKIPPED\u001b[0m (...)\n",
      "tests/test_tokenizer.py::test_encode_memory_usage \u001b[33mSKIPPED\u001b[0m (rlimit su...)\n",
      "\n",
      "\u001b[32m======================== \u001b[32m\u001b[1m23 passed\u001b[0m, \u001b[33m2 skipped\u001b[0m\u001b[32m in 30.11s\u001b[0m\u001b[32m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv run pytest tests/test_tokenizer.py -v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
