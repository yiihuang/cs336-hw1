{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536b308f",
   "metadata": {},
   "source": [
    "# 1. Assignment Overview\n",
    "\n",
    "In this assignment, you will build all the components needed to train a standard Transformer language model (LM) from scratch and train some models.\n",
    "\n",
    "## What you will implement\n",
    "1. Byte-pair encoding (BPE) tokenizer (§2)\n",
    "2. Transformer language model (LM) (§3)\n",
    "3. The cross-entropy loss function and the AdamW optimizer (§4)\n",
    "4. The training loop, with support for serializing and loading model and optimizer state (§5)\n",
    "\n",
    "## What you will run\n",
    "1. Train a BPE tokenizer on the TinyStories dataset.\n",
    "2. Run your trained tokenizer on the dataset to convert it into a sequence of integer IDs.\n",
    "3. Train a Transformer LM on the TinyStories dataset.\n",
    "4. Generate samples and evaluate perplexity using the trained Transformer LM.\n",
    "5. Train models on OpenWebText and submit your attained perplexities to a leaderboard.\n",
    "\n",
    "## What you can use\n",
    "We expect you to build these components from scratch. In particular, you may **not** use any definitions from `torch.nn`, `torch.nn.functional`, or `torch.optim` except for the following:\n",
    "\n",
    "- `torch.nn.Parameter`\n",
    "- Container classes in `torch.nn` (e.g., `Module`, `ModuleList`, `Sequential`, etc.)\n",
    "- The `torch.optim.Optimizer` base class\n",
    "\n",
    "You may use any other PyTorch definitions. If you would like to use a function or class and are not sure whether it is permitted, feel free to ask on Slack. When in doubt, consider whether using it compromises the “from-scratch” ethos of the assignment.\n",
    "\n",
    "## Statement on AI tools\n",
    "\n",
    "Prompting LLMs such as ChatGPT is permitted for low-level programming questions or high-level conceptual questions about language models, but using it directly to solve the problem is **prohibited**.\n",
    "\n",
    "We strongly encourage you to disable AI autocomplete (e.g., Cursor Tab, GitHub Copilot) in your IDE when completing assignments (non-AI autocomplete, e.g., function-name completion, is fine). AI autocomplete often makes it harder to engage deeply with the content.\n",
    "\n",
    "## What the code looks like\n",
    "\n",
    "All the assignment code and this writeup are on GitHub:\n",
    "\n",
    "> https://github.com/stanford-cs336/assignment1-basics\n",
    "\n",
    "Please `git clone` the repo. If there are updates, we’ll notify you so you can `git pull` to get the latest.\n",
    "\n",
    "1. **`cs336_basics/*`**: Your code lives here. There’s no code provided—you can implement everything from scratch.\n",
    "2. **`adapters.py`**: Defines required functionality your code must expose. For each feature (e.g., scaled dot-product attention), fill in the corresponding hook (e.g., `run_scaled_dot_product_attention`) by invoking your implementation. *Do not put substantive logic here; it’s glue code.*\n",
    "3. **`test_*.py`**: Tests you must pass (e.g., `test_scaled_dot_product_attention`) that call hooks defined in `adapters.py`. **Do not edit** the tests.\n",
    "\n",
    "## How to submit\n",
    "\n",
    "You will submit the following files to Gradescope:\n",
    "\n",
    "- **`writeup.pdf`**: Answer all written questions. Please typeset your responses.\n",
    "- **`code.zip`**: Contains all the code you’ve written.\n",
    "\n",
    "To submit to the leaderboard, open a PR to:\n",
    "\n",
    "> https://github.com/stanford-cs336/assignment1-basics-leaderboard\n",
    "\n",
    "See the `README.md` in the leaderboard repository for detailed submission instructions.\n",
    "\n",
    "## Where to get datasets\n",
    "\n",
    "This assignment will use two pre-processed datasets: **TinyStories** [Eldan and Li, 2023] and **OpenWebText** [Gokaslan et al., 2019]. Both datasets are single, large plaintext files. If you are doing the assignment with the class, you can find these files at `/data` of any non-head node machine. If you are following along at home, you can download these files with the commands inside the `README.md`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca1cc7",
   "metadata": {},
   "source": [
    "> **Low-Resource/Downscaling Tip: Init**  \n",
    "> Throughout the course’s assignment handouts, we will give advice for working through parts of the assignment with fewer or no GPU resources. For example, we will sometimes suggest **downscaling** your dataset or model size, or explain how to run training code on a MacOS integrated GPU or CPU. You’ll find these “low-resource tips” in a blue box (like this one). Even if you are an enrolled Stanford student with access to the course machines, these tips may help you iterate faster and save time, so we recommend you read them!\n",
    "\n",
    "> **Low-Resource/Downscaling Tip: Assignment 1 on Apple Silicon or CPU**  \n",
    "> With the staff solution code, we can train an LM to generate reasonably fluent text on an Apple M3 Max chip with 36 GB RAM, in under **5 minutes** on Metal GPU (MPS) and about **30 minutes** using the CPU. If these words don’t mean much to you, don’t worry! Just know that if you have a reasonably up-to-date laptop and your implementation is correct and efficient, you will be able to train a small LM that generates simple children’s stories with decent fluency.  \n",
    "> Later in the assignment, we will explain what changes to make if you are on CPU or MPS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12368d27",
   "metadata": {},
   "source": [
    "# 2. Byte-Pair Encoding (BPE) Tokenizer\n",
    "\n",
    "In the first part of the assignment, we will train and implement a **byte-level** byte-pair encoding (BPE) tokenizer [Sennrich et al., 2016; Wang et al., 2019]. In particular, we will represent arbitrary (Unicode) strings as a sequence of **bytes** and train our BPE tokenizer on this byte sequence. Later, we will use this tokenizer to encode text (a string) into **tokens** (a sequence of integers) for language modeling.\n",
    "\n",
    "## 2.1 The Unicode Standard\n",
    "\n",
    "Unicode is a text encoding standard that maps characters to integer **code points**. As of Unicode 16.0 (released in September 2024), the standard defines **154,998** characters across **168** scripts. For example, the character “s” has the code point **115** (typically notated as `U+0073`, where `U+` is a conventional prefix and `0073` is 115 in hexadecimal), and the character “牛” has the code point **29275**.  \n",
    "In Python, you can use the `ord()` function to convert a single Unicode character into its integer representation. The `chr()` function converts an integer Unicode code point into a string with the corresponding character.\n",
    "\n",
    "```py\n",
    ">>> ord('牛')\n",
    "29275\n",
    ">>> chr(29275)\n",
    "'牛'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d8aa3",
   "metadata": {},
   "source": [
    "Problem (unicode1): Understanding Unicode (1 point)\n",
    "1. What Unicode character does `chr(0)` return?\n",
    "Deliverable: A one-sentence response.\n",
    "2. How does this character’s string representation `(__repr__())` differ from its printed representation?\n",
    "Deliverable: A one-sentence response.\n",
    "3. What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b0cec31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6eed842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6ac5d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72528426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38cb70",
   "metadata": {},
   "source": [
    "## 2.2 Unicode Encodings\n",
    "\n",
    "While the Unicode standard defines a mapping from characters to code points (integers), it’s impractical to train tokenizers directly on Unicode codepoints, since the vocabulary would be prohibitively large (around 150K items) and sparse (since many characters are quite rare). Instead, we’ll use a Unicode **encoding**, which converts a Unicode character into a sequence of **bytes**. The Unicode standard itself defines three encodings: **UTF-8**, **UTF-16**, and **UTF-32**, with **UTF-8** being the dominant encoding for the Internet (more than 98% of all webpages).\n",
    "\n",
    "To encode a Unicode string into UTF-8, we can use the `encode()` function in Python. To access the underlying byte values for a Python `bytes` object, we can iterate over it (e.g., call `list()`). Finally, we can use the `decode()` function to decode a UTF-8 byte string into a Unicode string.\n",
    "\n",
    "```py\n",
    ">>> test_string = \"hello! こんにちは!\"\n",
    ">>> utf8_encoded = test_string.encode(\"utf-8\")\n",
    ">>> print(utf8_encoded)\n",
    "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
    ">>> print(type(utf8_encoded))\n",
    "<class 'bytes'>\n",
    "\n",
    "# Get the byte values for the encoded string (integers from 0 to 255).\n",
    ">>> list(utf8_encoded)\n",
    "[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33]\n",
    "\n",
    "# One byte does not necessarily correspond to one Unicode character!\n",
    ">>> print(len(test_string))\n",
    "13\n",
    ">>> print(len(utf8_encoded))\n",
    "23\n",
    "\n",
    ">>> print(utf8_encoded.decode(\"utf-8\"))\n",
    "hello! こんにちは!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449df3b8",
   "metadata": {},
   "source": [
    "By converting our Unicode codepoints into a sequence of bytes (e.g., via the UTF-8 encoding), we\n",
    "are essentially taking a sequence of codepoints (integers in the range 0 to 154,997) and transforming it\n",
    "into a sequence of byte values (integers in the range 0 to 255). The 256-length byte vocabulary is much\n",
    "more manageable to deal with. When using byte-level tokenization, we do not need to worry about out-of-\n",
    "vocabulary tokens, since we know that any input text can be expressed as a sequence of integers from 0 to\n",
    "255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e04587",
   "metadata": {},
   "source": [
    "> **Problem (unicode2): Unicode Encodings (3 points)**\n",
    "\n",
    "**(a)** What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.  \n",
    "**Deliverable:** A one-to-two sentence response.\n",
    "\n",
    "**(b)** Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.\n",
    "\n",
    "```python\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    ">>> decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))\n",
    "'hello'\n",
    "```\n",
    "**Deliverable:** An example input byte string for which `decode_utf8_bytes_to_str_wrong` produces incorrect output, with a one-sentence explanation of why the function is incorrect.\n",
    "\n",
    "**(c)** Give a two byte sequence that does not decode to any Unicode character(s).\n",
    "\n",
    "**Deliverable:** An example, with a one-sentence explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ddb801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello \\xe9\\xbb\\x84\\xe5\\xa5\\x95!'\n",
      "<class 'bytes'>\n",
      "9 13\n",
      "hello 黄奕!\n"
     ]
    }
   ],
   "source": [
    "# try encoding a string with utf-8\n",
    "test_string = \"hello 黄奕!\"  # try your name\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)  # output: b'hello \\xe9\\xbb\\x84\\xe5\\xa5\\x95!'\n",
    "print(type(utf8_encoded)) \n",
    "\n",
    "# see the byte values\n",
    "list(utf8_encoded)  \n",
    "\n",
    "# verify the reversibility\n",
    "print(len(test_string), len(utf8_encoded))  \n",
    "print(utf8_encoded.decode(\"utf-8\"))  # perfectly restored: 'hello 黄奕!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52ebfc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc067975",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcafé\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mdecode_utf8_bytes_to_str_wrong\u001b[0;34m(bytestring)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mbytes\u001b[39m([b])\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"café\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a458018",
   "metadata": {},
   "source": [
    "## Why the function is incorrect:\n",
    "\n",
    "The function `decode_utf8_bytes_to_str_wrong` is incorrect because **UTF-8 is a variable-length encoding**. Some characters require multiple bytes to represent them, but the function tries to decode each individual byte as if it were a complete UTF-8 character.\n",
    "\n",
    "## The problem with \"café\":\n",
    "\n",
    "1. **\"café\"** in UTF-8 is encoded as: `b'caf\\xc3\\xa9'`\n",
    "2. The bytes are: `[99, 97, 102, 195, 169]`\n",
    "3. The first three bytes (`c`, `a`, `f`) are ASCII characters that can be decoded individually\n",
    "4. However, the **é** character requires **2 bytes**: `0xc3` and `0xa9`\n",
    "   - `0xc3` is the first byte of a 2-byte UTF-8 sequence, but when decoded alone, it's incomplete\n",
    "   - `0xa9` is the second byte, but when decoded alone, it's not a valid UTF-8 start byte\n",
    "\n",
    "## The correct approach:\n",
    "\n",
    "Instead of decoding each byte individually, you should decode the entire byte string at once:\n",
    "\n",
    "```python\n",
    "def decode_utf8_bytes_to_str_correct(bytestring: bytes):\n",
    "    return bytestring.decode(\"utf-8\")\n",
    "```\n",
    "\n",
    "This works because the UTF-8 decoder knows how to properly interpret the multi-byte sequences and reconstruct the complete Unicode characters.\n",
    "\n",
    "The error you're seeing (`'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data`) occurs because `0xc3` is the start of a 2-byte UTF-8 sequence, but when you try to decode it as a single byte, the decoder expects more data that isn't there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "233a80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf8_bytes_to_str_correct(bytestring: bytes):\n",
    "    return bytestring.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd63d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'café'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_correct(\"café\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919078c",
   "metadata": {},
   "source": [
    "## 2.3 Subword Tokenization\n",
    "\n",
    "While byte-level tokenization can alleviate the out-of-vocabulary issues faced by word-level tokenizers, tokenizing text into **bytes** results in extremely long input sequences. This slows down model training: a sentence with 10 words might be ~10 tokens in a word-level LM, but could be 50+ tokens in a character/byte-level model (depending on word lengths). Longer sequences increase compute per step, and modeling long byte sequences creates longer-term dependencies.\n",
    "\n",
    "**Subword tokenization** sits between word-level and byte-level tokenizers.\n",
    "\n",
    "- A byte-level tokenizer’s vocabulary has **256** entries (byte values 0–255).\n",
    "- A subword tokenizer trades a **larger vocabulary** for **shorter sequences** (better compression).\n",
    "- Example: if the byte sequence `b'the'` occurs frequently, assigning it its own token reduces a 3-byte sequence to **one** token.\n",
    "\n",
    "**Choosing subword units.**  \n",
    "Sennrich et al. (2016), following Gage (1994), propose **byte-pair encoding (BPE)**—a compression algorithm that iteratively merges the most frequent adjacent pair into a new token. Repeated merges add subword tokens to maximize compression: frequent words may become single subword units.\n",
    "\n",
    "Subword tokenizers built via BPE are often called **BPE tokenizers**. In this assignment, we implement a **byte-level BPE** tokenizer: vocabulary items are bytes or merged byte sequences, combining robust OOV handling with manageable sequence lengths. Constructing the BPE vocabulary is called **“training”** the tokenizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a036c0",
   "metadata": {},
   "source": [
    "## 2.4 BPE Tokenizer Training\n",
    "\n",
    "The BPE tokenizer training procedure consists of three main steps.\n",
    "\n",
    "### Vocabulary initialization\n",
    "The tokenizer vocabulary is a one-to-one mapping from bytestring token to integer ID. Since we are training a **byte-level** BPE tokenizer, our initial vocabulary is simply the set of all bytes. Because there are 256 possible byte values, the initial vocabulary size is **256**.\n",
    "\n",
    "### Pre-tokenization\n",
    "Once you have a vocabulary, you could, in principle, count how often bytes occur next to each other in your text and begin merging them starting with the most frequent pair of bytes. However, this is quite computationally expensive, since you would have to take a full pass over the corpus for each merge. In addition, directly merging bytes across the corpus may result in tokens that differ only in punctuation (e.g., “dog!” vs. “dog.”), which would get completely different token IDs even though they are highly semantically similar.\n",
    "\n",
    "To avoid this, we **pre-tokenize** the corpus. You can think of this as a coarse-grained tokenization over the corpus that helps us count how often pairs of characters appear. For example, the word `'text'` might be a pre-token that appears 10 times. In this case, when we count how often the characters `t` and `e` appear next to each other, we will see that the word `text` has `t` and `e` adjacent and we can increment their count by 10 instead of scanning the whole corpus. Since we’re training a byte-level BPE model, each pre-token is represented as a sequence of UTF-8 bytes.\n",
    "\n",
    "The original BPE implementation of Sennrich et al. (2016) pre-tokenizes by simply splitting on whitespace (i.e., `s.split(\" \")`). In contrast, we will use a **regex-based** pre-tokenizer (used by GPT-2; Radford et al., 2019) from the tiktoken project:  \n",
    "`https://github.com/openai/tiktoken/pull/234/files`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed015df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473f9c7",
   "metadata": {},
   "source": [
    "It may be useful to interactively split some text with this pre-tokenizer to get a better sense of its behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4763815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requires `regex` package\n",
    "import regex as re\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")\n",
    "# -> ['some', ' ', 'text', ' ', 'that', ' ', 'i', \"'ll\", ' ', 'pre', '-', 'tokenize']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684bc19",
   "metadata": {},
   "source": [
    "When using it in your code, however, you should use `re.finditer` to avoid storing the pre-tokenized words\n",
    "as you construct your mapping from pre-tokens to their counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654b9e4",
   "metadata": {},
   "source": [
    "### Compute BPE merges\n",
    "\n",
    "Now that we’ve converted our input text into **pre-tokens** and represented each pre-token as a sequence of **UTF-8 bytes**, we can compute the BPE merges (i.e., train the BPE tokenizer).\n",
    "\n",
    "At a high level, the **BPE algorithm** iteratively:\n",
    "1. Counts every adjacent **pair of bytes**.\n",
    "2. Finds the pair with the **highest frequency** (e.g., `(\"A\",\"B\")`).\n",
    "3. **Merges** every occurrence of this pair into a new token (e.g., `\"AB\"`).\n",
    "4. Adds the new token to the **vocabulary**.\n",
    "\n",
    "The final vocabulary size equals the initial size (**256** for byte-level) **plus** the number of merge operations performed.  \n",
    "For efficiency, **do not consider pairs that cross pre-token boundaries**.  \n",
    "When ties occur in pair frequency, **break ties deterministically by choosing the lexicographically greater pair**. Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78d146e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BA', 'A')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dd1849e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'st'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(\"es\"),(\"st\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9ed32e",
   "metadata": {},
   "source": [
    "### Special tokens\n",
    "\n",
    "Some strings (e.g., `<|endoftext|>`) encode metadata such as document boundaries. When encoding text, it’s often desirable to mark such strings as **special tokens** that **must never be split** and are **always preserved as a single token** (one integer ID). This ensures, for example, that an EOS token is recognized unambiguously during generation.\n",
    "These special tokens must be **added to the vocabulary** with **fixed IDs**.\n",
    "\n",
    "> *Reference:* Algorithm 1 of **Sennrich et al. (2016)** gives an (inefficient) BPE training procedure that essentially follows the steps above. Implementing and testing that function is a useful first exercise to check your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffddba87",
   "metadata": {},
   "source": [
    "\n",
    "### Example (`bpe_example`): BPE training example\n",
    "\n",
    "Here is a stylized example from *Sennrich et al.* (2016). Consider a corpus consisting of the following text:\n",
    "\n",
    "```\n",
    "\n",
    "low low low low low\n",
    "lower lower widest widest widest\n",
    "newest newest newest newest newest newest\n",
    "\n",
    "```\n",
    "\n",
    "and the vocabulary has a special token `<|endoftext|>`.\n",
    "\n",
    "**Vocabulary**  \n",
    "We initialize our vocabulary with `<|endoftext|>` and the 256 byte values.\n",
    "\n",
    "**Pre-tokenization**  \n",
    "For simplicity (to focus on the merge procedure), pretokenization splits on whitespace. Counting tokens yields the frequency table:\n",
    "\n",
    "`{low: 5, lower: 2, widest: 3, newest: 6}`\n",
    "\n",
    "It’s convenient to represent this as a `dict[tuple[bytes], int]`, e.g. `{(l,o,w): 5, …}`.  \n",
    "Note: even a single byte is a `bytes` object in Python. There is no separate `byte` type for a single byte, just as there is no `char` type in Python for a single character.\n",
    "\n",
    "**Merges**  \n",
    "First, look at every successive pair of bytes and sum the frequency across words where they appear:\n",
    "\n",
    "`{lo: 7, ow: 7, we: 8, er: 2, wi: 3, id: 3, de: 3, es: 9, st: 9, ne: 6, ew: 6}`\n",
    "\n",
    "The pairs `('es')` and `('st')` are tied; choose the lexicographically greater pair, `('st')`.  \n",
    "After this merge the pre-tokens become:  \n",
    "`{(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,e,st): 3, (n,e,w,e,st): 6}`.\n",
    "\n",
    "Second round: `(e, st)` is most common (count = 9); merge to get:  \n",
    "`{(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,est): 3, (n,e,w,est): 6}`.\n",
    "\n",
    "Continuing, if we take **6 merges**, the sequence is:  \n",
    "`['s t', 'e st', 'o w', 'l ow', 'w est', 'n e']`.\n",
    "\n",
    "With these merges, the vocabulary elements include:  \n",
    "`[<|endoftext|>, […256 BYTE CHARS], st, est, ow, low, west, ne]`.\n",
    "\n",
    "With this vocabulary and merge list, the word **`newest`** tokenizes as:  \n",
    "`[ne, west]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eee7e4",
   "metadata": {},
   "source": [
    "### 2.5 Experimenting with BPE Tokenizer Training\n",
    "\n",
    "Let’s train a byte-level BPE tokenizer on the TinyStories dataset. Instructions to find/download the dataset are in Section 1. Before you start, skim TinyStories to get a sense of what’s in the data.\n",
    "\n",
    "**Parallelizing pre-tokenization**  \n",
    "A major bottleneck is pre-tokenization. You can speed this up by parallelizing using Python’s built-in `multiprocessing`. In parallel implementations, **chunk the corpus so that chunk boundaries occur at the beginning of a special token**. You can copy the starter code here to compute chunk boundaries and distribute work:\n",
    "\n",
    "<https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py>\n",
    "\n",
    "This chunking is always valid because we never merge across document boundaries. For this assignment, you can always split in this way. Don’t worry about the edge case of a huge corpus that lacks `<|endoftext|>`.\n",
    "\n",
    "**Removing special tokens before pre-tokenization**  \n",
    "Before running pre-tokenization with a regex pattern (e.g., using `re.finditer`), strip out all special tokens from your corpus (or from each chunk in a parallel implementation). **Split on the special tokens** so no merging can occur across the document boundary. This can be done with `re.split` using `\"|\" .join(special_tokens)` as the delimiter (**make sure to use `re.escape` since `|` occurs in the special tokens**). The test `test_train_bpe_special_tokens` checks this.\n",
    "\n",
    "**Optimizing the merging step**  \n",
    "The naïve BPE training loop is slow because it recomputes frequencies over all byte pairs each round. Speed it up by **indexing the counts of all pairs** and **incrementally updating** only the pairs that overlap the most recently merged pair. This caching yields significant speedups, although the **merging phase itself is not parallelizable in Python**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f8bf29",
   "metadata": {},
   "source": [
    "### Low-Resource/Downscaling Tip: Profiling\n",
    "\n",
    "Use profiling tools like **cProfile** or **scalene** to identify bottlenecks in your implementation and focus your optimization there.\n",
    "\n",
    "---\n",
    "\n",
    "### Low-Resource/Downscaling Tip: “Downscaling”\n",
    "\n",
    "Instead of immediately training on the full TinyStories dataset, start with a **debug dataset** (a small subset).  \n",
    "Example: train on the **TinyStories validation set** (≈22K documents) instead of 2.12M.  \n",
    "This general strategy speeds up development (smaller datasets, smaller models, etc.).  \n",
    "Choose the debug size carefully: **large enough** to surface the same bottlenecks as the full setup (so optimizations generalize), but **not so large** that runs take forever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c96b6",
   "metadata": {},
   "source": [
    "### Problem (`train_bpe`): BPE Tokenizer Training *(15 points)*\n",
    "\n",
    "**Deliverable**  \n",
    "Write a function that, given a path to an input text file, trains a **byte-level BPE tokenizer**.  \n",
    "Your function should accept at least:\n",
    "\n",
    "- `input_path: str` — Path to a text file with BPE tokenizer training data.  \n",
    "- `vocab_size: int` — Maximum final vocabulary size (includes initial byte vocabulary, merges, and any special tokens).  \n",
    "- `special_tokens: list[str]` — Strings to add to the vocabulary; these do **not** otherwise affect BPE training.\n",
    "\n",
    "Return the resulting vocabulary and merges:\n",
    "\n",
    "- `vocab: dict[int, bytes]` — Tokenizer vocabulary mapping token ID (`int`) → token bytes (`bytes`).  \n",
    "- `merges: list[tuple[bytes, bytes]]` — BPE merges produced during training.  \n",
    "  Each item is a tuple `(<token1>, <token2>)`, meaning `<token1>` was merged with `<token2>`.  \n",
    "  **Order merges by creation time.**\n",
    "\n",
    "**Testing**  \n",
    "To run our tests, first implement the test adapter at `adapters.run_train_bpe`.  \n",
    "Then run:\n",
    "```bash\n",
    "uv run pytest tests/test_train_bpe.py\n",
    "````\n",
    "\n",
    "Your implementation should pass all tests.\n",
    "\n",
    "**Optional speedups**\n",
    "You may implement hot paths in a systems language:\n",
    "\n",
    "* **C++** (consider **cppyy**) or **Rust** (using **PyO3**).\n",
    "  Be mindful of copy vs. zero-copy from Python memory. Provide build instructions or ensure it builds with only `pyproject.toml`.\n",
    "\n",
    "**Regex note**\n",
    "The **GPT-2 regex** is not well supported (and often too slow) in many engines.\n",
    "We verified **Oniguruma** is reasonably fast and supports negative lookahead, but Python’s **`regex`** package is—if anything—even faster.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6338eaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yhuang/cs336-hw1/.venv/bin/python: No module named uv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest tests/test_train_bpe.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
